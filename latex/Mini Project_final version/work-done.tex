\chapter{Duplicate file remover}

The duplicate file remover is a code designed with the objective to remove duplicate instances of file in any directory and its sub directories. The program uses the MD5 hash function to hash the files present in the directories. It then compares the files using the hash values and then deletes the files present in the directory which are duplicates. 
\\~\\
The code has been written in python programming language. Python provides a set of libraries like the md5 library which contains the md5 hash function.  
\\~\\It also contains the library hashlib which contain the all the hash functions like md5 , sha-1 , sha-2.
\\~\\Besides these, the program makes use of os library which contains functions required for traversing the directory structure of the operating system. 
\\~\\The program uses a graphical user interface and takes in the file path as input.\\~\\
Python provides a data structuring mechanism known as dictionary. This program makes use of this data structure to map the size of data to the files having the particular size so that we can create a structuring for files so as to perform further operations.\\~\\
After creating a dictionary we put in files in the dictionary. If size of the files are same then the size is taken as the key and is initialized as a list containing the names of files having same size.\\
\\
The os.path.walk(path\_name,function, arguments)  takes in as argument as file directory which has to be traversed. It then traverses each and every file present in the directory and sub directories executing the function in the argument for each file along with the arguments passed to the function being called each time.\\~\\
In the next phase the files are sorted by size on the basis of keys and the sorted keys are stored in an array. These keys are used to reference files of same size one by one and then these files are checked for duplicity in the first 2048 bytes.\\~\\
 The MD5 hash function is used to perform this task. If the match is not found then filenames are removed from the list of potential duplicates. Else the files are flagged true for further checking. The initial dictionary is deleted as it is not required and the output array of files containing potential duplicate are then passed on to the next checking phase.\\~\\
In the last checking phase, files in the potential duplicate are checked completely by reading data chunks of 4096 bytes each time for better file reading and preventing buffer overflow. At this stage the duplicate files are permanently deleted from the memory.


