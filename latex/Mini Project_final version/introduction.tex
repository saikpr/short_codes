\cleardoublepage
\phantomsection
\addcontentsline{toc}{chapter}{References}
\chapter{Introduction}


File comparison has always been an important task since the advent of computing.\\~\\
With ever increasing network performance and more and more computers getting online large amounts of duplicate data  from different sources keep on getting accumulated in local systems. Also due to improved methods of data transfers and increasing local storage space, users generally do not care about removing these duplicate files from their systems. 
\\~\\
However, along with the increase of data storage size one more thing that is increasing is the size of the data itself. Earlier an image file used to be of size comparable to 100 KiloBytes. Now a single 5 MP image typically contains over 10 Megabytes of data. Thus, the problem of having duplicate file filling up hard disk space has not been completely mitigated.\\~\\ 
Secondly, the more data a hard drive contains the more are its chances of early failure due to issues like larger access time , hard drive data fragmentation and also extra active time of the hard drive during reading and writing.\\~\\
Two files are considered duplicates if they contain exactly same data. The file names may be different. The presented utility application removes the two files which are exactly same using techniques of cryptography and hashing to compare two files. This program is robust and easy to use for general purpose.\\~\\